# -*- coding: utf-8 -*-
"""B21CS070_InLab_Assignment9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IFZRKhA-A8rHQyKGK0HD5C6pD49hjLRQ

# Lab 8
"""

import numpy as np
import pandas as pd
import torch
import torchvision
import torchvision.datasets as dataset
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split as tts
from torchvision import transforms
import torch.nn as nn
import torch.optim as optim
import copy
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
import random
import seaborn as sns
import pickle

"""## Problem 1

### Part 1
"""

data_test= dataset.MNIST(root='./data', train=False, download=True,transform = None)
data_train = dataset.MNIST(root='./data', train=True, download=True,transform = None)

Y = data_train.targets.numpy()
X = data_train.data.numpy()

mean_train = data_train.data.float().mean() / 255
std_train = data_train.data.float().std() / 255
mean_test = data_test.data.float().mean() / 255
std_test = data_test.data.float().std() / 255

mean_train,std_train,mean_test,mean_test

X_train,X_val,Y_train,Y_val = tts(X,Y,test_size = 0.2)
X_train.shape

train_transforms = transforms.Compose([transforms.RandomRotation(5),transforms.RandomCrop(28, padding=2),transforms.ToTensor(),transforms.Normalize(mean=[mean_train], std=[std_train])])

test_transforms = transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean=[mean_test], std=[std_test])])

train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=train_transforms)
val_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=test_transforms)
test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=test_transforms)

train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [48000, 12000])

"""### Part 2"""

class_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']
fig, axs = plt.subplots(10, 5, figsize=(10, 20))
for i in range(10):
    class_samples = np.random.choice(np.where(train_dataset.dataset.targets == i)[0], size=5, replace=False)
    for j, sample_idx in enumerate(class_samples):
        axs[i, j].imshow(train_dataset.dataset.data[sample_idx], cmap='gray')
        axs[i, j].axis('off')
        if j == 0:
            axs[i, j].set_title(class_names[i])
plt.tight_layout()
plt.show()

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=128, shuffle=False)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)

"""### Part 3"""

class MLP(nn.Module):
    def __init__(self,input_size,hidden_size,output_size):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = x.view(-1, 784)
        x = nn.functional.relu(self.fc1(x))
        x = nn.functional.relu(self.fc2(x))
        x = self.fc3(x)
        return x


model = MLP(784,256,10)

print(f'The number of trainable parameters is {sum(p.numel() for p in model.parameters() if p.requires_grad):,}')

"""### Part 4"""

def run_epochs(num_epochs,criterion,optimizer,model):
  best_model_wts = copy.deepcopy(model.state_dict())
  best_acc = 0.0
  train_losses, train_accs = [], []
  val_losses, val_accs = [], []

  for epoch in range(num_epochs):   ## Running for 5 epochs
      
      model.train()
      train_loss = 0.0
      train_correct = 0
      for inputs, targets in train_loader:
          optimizer.zero_grad()
          outputs = model(inputs)
          loss = criterion(outputs, targets)
          loss.backward()
          optimizer.step()

          train_loss += loss.item() * inputs.size(0)
          train_correct += torch.sum(torch.argmax(outputs, dim=1) == targets)

      train_loss /= len(train_loader.dataset)
      train_acc = train_correct / len(train_loader.dataset)
      train_losses.append(train_loss)
      train_accs.append(train_acc)

      model.eval()
      val_loss = 0.0
      val_correct = 0
      with torch.no_grad():
          for inputs, targets in val_loader:
              outputs = model(inputs)
              loss = criterion(outputs, targets)

              val_loss += loss.item() * inputs.size(0)
              val_correct += torch.sum(torch.argmax(outputs, dim=1) == targets)

      val_loss /= len(val_loader.dataset)
      val_acc = val_correct / len(val_loader.dataset)
      val_losses.append(val_loss)
      val_accs.append(val_acc)


      if val_acc > best_acc:
          best_acc = val_acc
          best_model_wts = copy.deepcopy(model.state_dict())

    
      print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, '
            f'Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')


  model.load_state_dict(best_model_wts)
  print('Best Val Acc: {:.4f}'.format(best_acc))
  return train_losses,train_accs,val_losses,val_accs

num_epochs = 5

criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters())

train_losses,train_accs,val_losses,val_accs  = run_epochs(num_epochs,criterion,optimizer,model)

"""### Part 5

"""

model.eval()
test_preds = []
test_targets = []
with torch.no_grad():
    for inputs, targets in test_loader:
        outputs = model(inputs)
        preds = torch.argmax(outputs, dim=1)
        test_preds.extend(preds.cpu().numpy())
        test_targets.extend(targets.cpu().numpy())

correct_indices = []
incorrect_indices = []
for i in range(len(test_dataset)):
    if test_preds[i] == test_targets[i]:
        if len(correct_indices) < 5:
            correct_indices.append(i)
    else:
        if len(incorrect_indices) < 5:
            incorrect_indices.append(i)
    if len(correct_indices) == 5 and len(incorrect_indices) == 5:
        break


fig, axs = plt.subplots(1, 5, figsize=(10, 3))
for i, idx in enumerate(correct_indices):
    img, true_label = test_dataset[idx]
    pred_label = test_dataset.classes[test_preds[idx]]
    axs[i].imshow(img.squeeze().numpy(), cmap='gray')
    axs[i].set(title=f"True: {true_label}\nPredicted: {pred_label}")
plt.suptitle("Correct Predictions", fontsize=20)
plt.show()


fig, axs = plt.subplots(1, 5, figsize=(10, 3))
for i, idx in enumerate(incorrect_indices):
    img, true_label = test_dataset[idx]
    pred_label = test_dataset.classes[test_preds[idx]]
    axs[i].imshow(img.squeeze().numpy(), cmap='gray')
    axs[i].set(title=f"True: {true_label}\nPredicted: {pred_label}")
plt.suptitle("Incorrect Predictions", fontsize=20)
plt.show()


plt.rcParams["figure.figsize"] = (10,5)
plt.subplot(1,2,1),
plt.plot(train_losses, label='Training loss')
plt.plot(val_losses, label='Validation loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1,2,2),
plt.plot(train_accs, label='Training accuracy')
plt.plot(val_accs, label='Validation accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""## Problem 2

### Part 1
"""

names = ["Sex",	"Length",	"Diameter",	"Height","Whole weight","Shucked weight","Viscera weight","Shell weight",	"Rings"]
abalone_df = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data",names = names)
abalone_df

abalone_df.describe()

abalone_df.isnull().sum()

for i in range(len(abalone_df["Height"])):
  if(abalone_df["Height"][i]==0):
    print(i)

X = abalone_df.copy()
X["Sex"] = LabelEncoder().fit_transform(X["Sex"])
X_scale = StandardScaler().fit_transform(X.drop(["Sex","Rings"],axis = 1))
X_scale = pd.DataFrame(data = X_scale,columns = names[1:-1])
X_scale["Sex"] = X["Sex"]
X_scale["Rings"] = X["Rings"]
X_scale = X_scale[names]
X_scale

sns.pairplot(data = X_scale,hue = "Rings")

class_no=[]
for n_rings in X_scale['Rings']:
    if(n_rings<8):
        class_no.append(0)
    elif(n_rings<13):
        class_no.append(1)
    elif(n_rings<18):
        class_no.append(2)
    else:
        class_no.append(3)

X_scale['Rings']=class_no
def stratify_split(X, Y, test_size=0.2, random_state=None):
    classes = np.unique(Y)
    train_indices, test_indices = [], []
    for c in classes:
        class_indices = np.where(Y == c)[0]
        n_class = len(class_indices)
        if n_class == 1:
            # Handle classes with only one example by including it in both sets
            train_indices.append(class_indices[0])
            test_indices.append(class_indices[0])
        else:
            n_test = int(np.ceil(n_class * test_size))
            n_train = n_class - n_test
            if random_state:
                np.random.seed(random_state)
            np.random.shuffle(class_indices)
            train_indices.extend(class_indices[:n_train])
            test_indices.extend(class_indices[n_train:])
    return X[train_indices], Y[train_indices], X[test_indices], Y[test_indices]

X_train,Y_train,X_test,Y_test = stratify_split(X_scale.drop("Rings",axis= 1).to_numpy(),X_scale["Rings"].to_numpy())

"""### Part 2"""

class MulticlassPerceptron:
    def __init__(self, n_input, n_hidden1, n_hidden2, n_output, learning_rate=0.01,af = 'sigmoid',wt = 'random'):
        self.n_input = n_input
        self.n_hidden1 = n_hidden1
        self.n_hidden2 = n_hidden2
        self.n_output = n_output
        self.learning_rate = learning_rate
        self.af = af
        # Initialize weights with random values

        
        self.W1 = np.random.randn(self.n_input, self.n_hidden1)-0.5
        self.b1 = np.zeros((1, self.n_hidden1))
        self.W2 = np.random.randn(self.n_hidden1, self.n_hidden2)-0.5
        self.b2 = np.zeros((1, self.n_hidden2))
        self.W3 = np.random.randn(self.n_hidden2, self.n_output)-0.5
        self.b3 = np.zeros((1, self.n_output))

        if(wt == 'zero'):
          self.w1 = np.zeros((self.n_input, self.n_hidden1)) 
          self.w2 = np.zeros((self.n_hidden1, self.n_hidden2))
          self.w3 = np.zeros((self.n_hidden2, self.n_output))
        if(wt == 'constant'):
          self.w1 = np.full((self.n_input, self.n_hidden1),0.1) 
          self.w2 = np.full((self.n_hidden1, self.n_hidden2),0.1)
          self.w3 = np.full((self.n_hidden2, self.n_output),0.1)
    
    def afunc(self,Z):
      if(self.af == 'sigmoid'):
        return  (1 / (1 + np.exp(-Z)))
      elif(self.af == 'relu'):
        return np.maximum(0, Z)
      else:
        return np.tanh(Z)
    def adfunc(self,Z):
      if(self.af == 'sigmoid'):
        sigmoid_x = 1 / (1 + np.exp(-Z))
        return sigmoid_x * (1 - sigmoid_x)

      if(self.af == 'relu'):
        return (Z > 0).astype(int)

      if(self.af == "tanh"):
        return 1 - np.tanh(Z)**2
    
    def softmax(self, x):
      exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
      return exp_x / np.sum(exp_x, axis=1, keepdims=True)
    
    def forward_propagation(self, X):
        # Input layer to hidden layer 1
        self.z1 = np.dot(X, self.W1) + self.b1
        self.a1 = self.afunc(self.z1)
        
        # Hidden layer 1 to hidden layer 2
        self.z2 = np.dot(self.a1, self.W2) + self.b2
        self.a2 = self.afunc(self.z2)
        
        # Hidden layer 2 to output layer
        self.z3 = np.dot(self.a2, self.W3) + self.b3
        self.y_hat = self.softmax(self.z3)
        
        return self.y_hat
    def backward_propagation(self, X, y, y_hat):
        # Output layer to hidden layer 2
        delta3 = y_hat
        y_adjusted = y - 1  # Adjust labels to start from 0
        delta3[range(X.shape[0]), y_adjusted] -= 1
        dW3 = np.dot(self.a2.T, delta3)
        db3 = np.sum(delta3, axis=0, keepdims=True)
        
        # Hidden layer 2 to hidden layer 1
        delta2 = np.dot(delta3, self.W3.T) * self.adfunc(self.z2)
        dW2 = np.dot(self.a1.T, delta2)
        db2 = np.sum(delta2, axis=0)
        
        # Hidden layer 1 to input layer
        delta1 = np.dot(delta2, self.W2.T) * self.adfunc(self.z1)
        dW1 = np.dot(X.T, delta1)
        db1 = np.sum(delta1, axis=0)
        
        # Update weights and biases
        self.W3 -= self.learning_rate * dW3
        self.b3 -= self.learning_rate * db3
        self.W2 -= self.learning_rate * dW2
        self.b2 -= self.learning_rate * db2
        self.W1 -= self.learning_rate * dW1
        self.b1 -= self.learning_rate * db1
    
    def predict(self, X):
        y_hat = self.forward_propagation(X)
        return np.argmax(y_hat, axis=1)
    
    # stochastic gradient descent
    def fit(self, X_train, y_train, n_epochs=100, batch_size=32):
        for epoch in range(n_epochs):
            # Shuffle training data
            idx = np.random.permutation(X_train.shape[0])
            X_train = X_train[idx]
            y_train = y_train[idx]
            
            # Mini-batch training
            for i in range(0, X_train.shape[0], batch_size):
                X_batch = X_train[i:i+batch_size]
                y_batch = y_train[i:i+batch_size]
                
                # Forward propagation
                y_hat = self.forward_propagation(X_batch)
                
                # Backward propagation
                self.backward_propagation(X_batch, y_batch, y_hat)
    def save_weights(self, file_path):
      weights = {
          'W1': self.W1,
          'b1': self.b1,
          'W2': self.W2,
          'b2': self.b2,
          'W3': self.W3,
          'b3': self.b3
      }
      with open(file_path, 'wb') as f:
          pickle.dump(weights, f)
      print(f'Saved weights to {file_path}')

    def load_weights(self, file_path):
      with open(file_path, 'rb') as f:
          weights = pickle.load(f)
      self.W1 = weights['W1']
      self.b1 = weights['b1']
      self.W2 = weights['W2']
      self.b2 = weights['b2']
      self.W3 = weights['W3']
      self.b3 = weights['b3']
      print(f'Loaded weights from {file_path}')

cnt = 0
arr1 = []
pp = MulticlassPerceptron(8,16,8,4,0.01,"tanh")
pp.fit(X_train,Y_train,100,100)
output = pp.predict(X_test)
for i in range(len(Y_test)):
    if(output[i]+1 == Y_test[i]):
      cnt += 1
print(cnt/len(Y_test))

"""### Part 3"""

arr1 = []
arr2 = []
arr3 = []
for i in range(100,501,100):
  cnt = 0
  pp = MulticlassPerceptron(8,16,8,4,0.01,"sigmoid")
  pp.fit(X_train,Y_train,i,100)
  output = pp.predict(X_test)
  for i in range(len(Y_test)):
      if(output[i]+1 == Y_test[i]):
        cnt += 1
  arr1.append(cnt/len(Y_test))
for i in range(100,501,100):
  cnt = 0
  pp = MulticlassPerceptron(8,16,8,4,0.01,"relu")
  pp.fit(X_train,Y_train,i,100)
  output = pp.predict(X_test)
  for i in range(len(Y_test)):
      if(output[i]+1 == Y_test[i]):
        cnt += 1
  arr2.append(cnt/len(Y_test))
for i in range(100,501,100):
  cnt = 0
  pp = MulticlassPerceptron(8,16,8,4,0.01,"tanh")
  pp.fit(X_train,Y_train,i,100)
  output = pp.predict(X_test)
  for i in range(len(Y_test)):
      if(output[i]+1 == Y_test[i]):
        cnt += 1
  arr3.append(cnt/len(Y_test))

plt.plot(range(100,501,100),arr1,color = 'r',label = 'sigmoid')
plt.plot(range(100,501,100),arr2,color = 'b',label = 'relu')
plt.plot(range(100,501,100),arr3,color = 'y',label = 'tanh')
plt.legend()
plt.xlabel("Iterations")
plt.ylabel("Accuracies")
plt.show()

"""### Part 4"""

arr1 = []
arr2 = []
arr3 = []
for i in range(100,501,100):
  cnt = 0
  pp = MulticlassPerceptron(8,16,8,4,0.01,"sigmoid",'random')
  pp.fit(X_train,Y_train,i,100)
  output = pp.predict(X_test)
  for i in range(len(Y_test)):
      if(output[i]+1 == Y_test[i]):
        cnt += 1
  arr1.append(cnt/len(Y_test))
for i in range(100,501,100):
  cnt = 0
  pp = MulticlassPerceptron(8,16,8,4,0.01,"sigmoid",'zero')
  pp.fit(X_train,Y_train,i,100)
  output = pp.predict(X_test)
  for i in range(len(Y_test)):
      if(output[i]+1 == Y_test[i]):
        cnt += 1
  arr2.append(cnt/len(Y_test))
for i in range(100,501,100):
  cnt = 0
  pp = MulticlassPerceptron(8,16,8,4,0.01,"sigmoid",'constant')
  pp.fit(X_train,Y_train,i,100)
  output = pp.predict(X_test)
  for i in range(len(Y_test)):
      if(output[i]+1 == Y_test[i]):
        cnt += 1
  arr3.append(cnt/len(Y_test))

plt.plot(range(100,501,100),arr1,color = 'r',label = 'random')
plt.plot(range(100,501,100),arr2,color = 'b',label = 'zero')
plt.plot(range(100,501,100),arr3,color = 'y',label = 'constant')
plt.legend()
plt.xlabel("Iterations")
plt.ylabel("Accuracies")
plt.show()

"""### Part 5"""

arr1 =[]
for i in range(16,129,16):
  pp = MulticlassPerceptron(8,i,i//2,4,0.01,"sigmoid",'random')
  pp.fit(X_train,Y_train,100,100)
  output = pp.predict(X_test)
  cnt = 0
  for i in range(len(Y_test)):
    if(output[i]+1 == Y_test[i]):
      cnt += 1
  arr1.append(cnt/len(Y_test))
plt.plot(range(16,129,16),arr1,color = 'r')
plt.xlabel("Hidden Nodes in first hidden layer")
plt.ylabel("Accuracies")
plt.show()

pp = MulticlassPerceptron(8,16,8,4,0.01,"sigmoid",'random')
pp.fit(X_train,Y_train,100,100)
output = pp.predict(X_test)
cnt = 0
for i in range(len(Y_test)):
  if(output[i]+1 == Y_test[i]):
    cnt += 1
arr1.append(cnt/len(Y_test))
pp.save_weights('model_weights.pkl')
pp.load_weights('model_weights.pkl')
with open('model_weights.pkl', 'rb') as f:
    weights = pickle.load(f)
print(weights)