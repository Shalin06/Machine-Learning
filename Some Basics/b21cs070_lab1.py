# -*- coding: utf-8 -*-
"""B21CS070_Lab1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oL7J4PPx2XWnzCK1AtgQZPBsld46VI0G

#**Q-1)**

##Q-1 a)
"""

import csv
with open("/content/drive/MyDrive/PRML/Lab1/Rollnumbers.csv",newline = '') as f:
    reader = csv.reader(f)
    data = list(reader)
print(data)

import pandas as pd
dataset = pd.read_csv("/content/drive/MyDrive/PRML/Lab1/Rollnumbers.csv")
arr = dataset.iloc[:,0]
print(arr)

"""## Q-1 b)"""

a = input()
print(int(a))

"""## Q-1 c)

"""

from datetime import datetime
datetime_str = '10/01/23 15:53:26'
ans = datetime.strptime(datetime_str, '%m/%d/%y %H:%M:%S')
print(ans)

"""## Q-1 d)"""

# external commands can be called using import os and subprocess
# import os
# in google collad we can use ! sign
!mkdir how

"""## Q-1 e)"""

#using .count
ls = ['1','1','1','5','6']
print(ls.count('1'))

"""## Q-1 f)"""

import numpy as np
ls =[[[1,2,3],[1,2,3],[1,2,89]]]
a = np.array(ls)
a.flatten()
print(list(a.flatten()))

"""## Q-1 g)"""

d1 = {'India': 'Delhi','Canada': 'Ottawa','United States': 'Washington D. C.'}
d2 = {'France': 'Paris','Malaysia': 'Kuala Lumpur'}
d3 = d1.copy()
d3.update(d2)
print(d3)

"""## Q-1 h)"""

ls = [1, 2, 4, 2, 1, 4, 5]
res = [*set(ls)]
print(res)

"""## Q-1 i)"""

d1 = {'India': 'Delhi','Canada': 'Ottawa','United States': 'Washington D. C.','Nepal' : 'Kathmandu'}
key = input()
if key in d1.keys():
  print("Key is present")
  print(key +':',d1[key])
else:
  print("Key is not present")

"""#**Q-2)**"""

import numpy as np
arr1 = np.array([[1,2],[4,5]])
arr2 = np.array([[7,8],[10,11]])
### part a) 
print("a) first row of first matrix")
print(arr1[0,:])
### part b)
print("b) second column of second matrix")
print(arr2[:,1])
### part c)
print("c) Matrix Multiplication")
print(arr1@arr2)
### part d)
print("d) element wise multiplication")
print(np.multiply(arr1,arr2))
### part e)
print("e) dot product")
print(np.dot(arr1,arr2))

"""#**Q-3)**

## Q-3 i)
"""

import pandas as pd
import numpy as np
df = pd.read_csv("/content/drive/MyDrive/PRML/Lab1/Cars93.csv")
## i)
# model     ---       nominal
# type      ---       ordinal
# max.price ---       ratio
# airbags   ---       ordinal

"""## Q-3 ii)"""

## ii) 
def handlenullvalues():
  newdf = df
  print(df.isnull().sum())
  for columns in df.columns:
    if(df[columns].isnull().sum() != 0):
      newdf[columns] = newdf[columns].fillna(0)
  print(newdf.isnull().sum())

  # value1 = np.mean(df['Luggage.room'])
  # value2 = np.mean(df['Rear.seat.room'])
  # newdf1 = df['Luggage.room'].fillna(value1)
  # newdf2 = df['Rear.seat.room'].fillna(value2)
  # print()
  # print("After making the value of null point as the mean of the column")
  # print()
  # print("mean of 'Luggage.room' is:   ",value1,'\n'"mean of 'Rear.seat.room' is: ",value2,'\n')
  # print(newdf1,'\n')
  # print(newdf2,'\n')
# print(df.isnull().sum())
handlenullvalues()

"""## Q-3 iv)"""

from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import normalize
lb = LabelEncoder()
newdf = df
column_headers = ['Type','AirBags','DriveTrain','Origin']
for i in column_headers:
  newdf[i]  = lb.fit_transform(df[i])
print(newdf)

"""## Q-3 iii)"""

## iii)
newdf  = df
def reducenoise():
  for headers in newdf.columns[2:]:
    for i in range(newdf[headers].size):
      try:
        newdf[headers][i] = newdf[headers][i].astype(float)
      except:
        newdf[headers][i] = "0"
reducenoise()
newdf

"""## Q-3 v)"""

headers = ['Type','AirBags','Max.Price']
for i in headers:
  newdf[i] = normalize([newdf[i]])[0]
print(newdf)

"""## Q-3 vi)"""

from sklearn.model_selection import train_test_split as tts
train,tempo = tts(df,test_size = 0.3,shuffle = True)
test,validate = tts(tempo,test_size = 2/3,shuffle = True)
validate

"""#**Q-4)**"""

from matplotlib import pyplot as plt 
import numpy as np

### part a)
x = np.linspace(-10,10,1000) 
plt.plot(x,5*x+4)
plt.xlabel('x')
plt.ylabel('y')
plt.title('a) y=5x + 4')
plt.show()

### part b)
x = np.linspace(10,100,1000)
plt.plot(x,np.log(x))
plt.xlabel('x')
plt.ylabel('y')
plt.title('b) y=ln')
plt.show()

### part c)
x = np.linspace(-10,10,1000)
plt.plot(x,x**2)
plt.xlabel('x')
plt.ylabel('y')
plt.title('c) y=x^2')
plt.show()

### part d)
x = [0,1,2,3,4]
y = [2,3,4,5,6]
plt.scatter(x,y)
plt.xlabel('x')
plt.ylabel('y')
plt.title('d) Scatter plot')
plt.show()

"""Q-5)"""



"""#**Q-5)**

## Import the Necessary Python Libraries and Components
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split as tts
from sklearn.linear_model import LogisticRegression as LR
from sklearn.tree import DecisionTreeClassifier as DTC
from sklearn.metrics import confusion_matrix as cm
from sklearn.metrics import precision_score as ps
from sklearn.metrics import recall_score as rs
from sklearn.metrics import f1_score as f1s
from sklearn.metrics import accuracy_score as acc

"""## To Disable Convergence Warnings (For Custom Training)"""

from warnings import simplefilter
from sklearn.exceptions import ConvergenceWarning
simplefilter("ignore", category=ConvergenceWarning)

"""## 1.) Input the Dataset"""

# Dataset Reference :- https://www.kaggle.com/uciml/breast-cancer-wisconsin-data

data = pd.read_csv("/content/drive/MyDrive/PRML/Lab1/Winconsin_Dataset.csv")
data

"""## 2.) Convert the String Labels into easily-interpretable Numerics"""

# Note :- There are many existing Encoders for converting String to Numeric Labels, but for convenience, we used Pandas.

condition_M = data.diagnosis == "M"
condition_B = data.diagnosis == "B"

data.loc[condition_M,"diagnosis"]=0
data.loc[condition_B,"diagnosis"]=1

data

"""## 3.) Converting Dataframe into Numpy Arrays (Features and Labels)"""

Y = data.diagnosis.to_numpy().astype('int')                                     # Labels

X_data = data.drop(columns=["id","diagnosis","Unnamed: 32"])
X = X_data.to_numpy()                                                           # Input Features

"""## 4.) Splitting the Dataset into Train and Test Portions"""

user_prompt = 0.3
user_enable = False

x_train,x_test,y_train,y_test = tts(X,Y,test_size=user_prompt,shuffle=user_enable)

"""## 5.) Model Training and Predicting"""

# Note :- Don't worry about the code snippet here, it is just to produce the predictions for the test data portion of each classifier

logistic_model = LR()
logistic_model.fit(x_train,y_train)
logistic_pred = logistic_model.predict(x_test)

decision_model = DTC()
decision_model.fit(x_train,y_train)
decision_pred = decision_model.predict(x_test)

"""## 6.) Evaluation Metrics (Inbulit v/s Scaratch)

### Confusion Matrix
"""

inbuilt_matrix_logistic = cm(y_test,logistic_pred)
inbuilt_matrix_decision = cm(y_test,decision_pred)

print("Confusion Matrix for Logistic Regression-based Predictions =>")
print(inbuilt_matrix_logistic)
print("Confusion Matrix for Decision Tree-based Predictions =>")
print(inbuilt_matrix_decision)

def confusion_matrix(y_test,y_pred):
  # Understand the Concept, write the code from scratch and remove "pass"
  arr = np.zeros([2,2],dtype = int)
  for i in range(len(y_test)):
    arr[y_test[i]][y_pred[i]] += 1
  return(arr)
print("Confusion Matrix for Logistic Regression-based Predictions =>")
print(confusion_matrix(y_test,logistic_pred))
print("Confusion Matrix for Decision Tree-based Predictions =>")
print(confusion_matrix(y_test,decision_pred))

"""### Average Accuracy"""

inbuilt_acc_logistic = acc(y_test,logistic_pred)
inbuilt_acc_decision = acc(y_test,decision_pred)

print("Accuracy for Logistic Regression-based Predictions =>",str(inbuilt_acc_logistic*100)+"%")
print("Accuracy for Decision Tree-based Predictions =>",str(inbuilt_acc_decision*100)+"%")

def avg_accuracy(arr):
  # Understand the Concept, write the code from scratch and remove "pass"
  val = ((arr[1][1]+arr[0][0])/(arr.sum()))*100
  return val;
print("Accuracy for Logistic Regression-based Predictions => "+str(avg_accuracy(confusion_matrix(y_test,logistic_pred)))+'%')
print("Accuracy for Decision Tree-based Predictions => "+str(avg_accuracy(confusion_matrix(y_test,decision_pred)))+'%')

"""### Precision"""

inbuilt_ps_logistic = ps(y_test,logistic_pred)
inbuilt_ps_decision = ps(y_test,decision_pred)

print("Precision for Logistic Regression-based Predictions =>",str(inbuilt_ps_logistic*100)+"%")
print("Precision for Decision Tree-based Predictions =>",str(inbuilt_ps_decision*100)+"%")

def precision(arr):
  # Understand the Concept, write the code from scratch and remove "pass"
  val = ((arr[1][1])/(arr[0][1]+arr[1][1]))*100
  return val;
print("Precision for Logistic Regression-based Predictions => " + str(precision(confusion_matrix(y_test,logistic_pred)))+'%')
print("Precision for Decision Tree-based Predictions => " + str(precision(confusion_matrix(y_test,decision_pred)))+'%')

"""### Recall"""

inbuilt_rs_logistic = rs(y_test,logistic_pred)
inbuilt_rs_decision = rs(y_test,decision_pred)

print("Recall for Logistic Regression-based Predictions =>",str(inbuilt_rs_logistic*100)+"%")
print("Recall for Decision Tree-based Predictions =>",str(inbuilt_rs_decision*100)+"%")

def recall(arr):
  # Understand the Concept, write the code from scratch and remove "pass"
  val = ((arr[1][1])/(arr[1][1]+arr[1][0]))*100
  return val
print("Recall for Logistic Regression-based Predictions => " + str(recall(confusion_matrix(y_test,logistic_pred)))+'%')
print("Recall for Decision Tree-based Predictions => " + str(recall(confusion_matrix(y_test,decision_pred)))+'%')

"""### F-1 Score"""

inbuilt_f1s_logistic = f1s(y_test,logistic_pred)
inbuilt_f1s_decision = f1s(y_test,decision_pred)

print("F1-Score for Logistic Regression-based Predictions =>",str(inbuilt_f1s_logistic*100)+"%")
print("F1-Score for Decision Tree-based Predictions =>",str(inbuilt_f1s_decision*100)+"%")

def f1_score(arr):
  # Understand the Concept, write the code from scratch and remove "pass"
  val = (2*recall(arr)*precision(arr))/(recall(arr)+ precision(arr))
  return val
print("F1-Score for Logistic Regression-based Predictions => " + str(f1_score(confusion_matrix(y_test,logistic_pred)))+ '%')
print("F1-Score for Decision Tree-based Predictions => " + str(f1_score(confusion_matrix(y_test,decision_pred)))+'%')

"""### Class-Wise Accuracy"""

def class_accuracy(arr):
  # Understand the Concept, write the code from scratch and remove "pass"
  val = ((arr[0][0]/(arr[0][0] + arr[0][1])) + (arr[1][1]/(arr[1][1]+arr[1][0])))/2
  return val*100
print("class accuracy for Logistic Regression-based Predictions => " + str(class_accuracy(confusion_matrix(y_test,logistic_pred)))+'%')
print("class accuracy for Decision Tree-based Predictions => "+ str(class_accuracy(confusion_matrix(y_test,decision_pred)))+'%')

"""### Sensitivity"""

def sensitivity(arr):
  # Understand the Concept, write the code from scratch and remove "pass"
  val = (arr[1][1]/(arr[1][1]+arr[1][0]))*100
  return val
print("sensitivity for Logistic Regression-based Predictions => " + str(sensitivity(confusion_matrix(y_test,logistic_pred)))+'%')
print("sensitivity for Decision Tree-based Predictions => "+ str(sensitivity(confusion_matrix(y_test,decision_pred)))+'%')

"""### Specificity"""

def specificity(arr):
  # Understand the Concept, write the code from scratch and remove "pass"
  val = (arr[0][0]/(arr[0][0]+arr[0][1]))*100
  return val
print("specificity for Logistic Regression-based Predictions => " + str(specificity(confusion_matrix(y_test,logistic_pred)))+'%')
print("specificity for Decision Tree-based Predictions => "+ str(specificity(confusion_matrix(y_test,decision_pred)))+'%')