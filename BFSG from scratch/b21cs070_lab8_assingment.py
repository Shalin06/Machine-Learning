# -*- coding: utf-8 -*-
"""B21CS070_Lab_Assignment8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Tw6EQPQjSEKVLLIJuPUrVb9JDtaJqJUR

# Lab 8
"""

!pip install mlxtend
!pip install mlxtend --upgrade --no-deps

import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split as tts
import random
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder
from sklearn import svm
from sklearn.neighbors import KNeighborsClassifier as KNN
from sklearn.ensemble import BaggingClassifier as BC
from sklearn.model_selection import cross_val_score as cvs
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestClassifier as RFC
from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score,confusion_matrix
from sklearn.naive_bayes import GaussianNB
import warnings
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
from sklearn.tree import DecisionTreeClassifier
warnings.filterwarnings('ignore')
import plotly.graph_objs as go
from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs

"""## Problem 1

### Part 1
"""

dataset = pd.read_csv("/content/drive/MyDrive/PRML/Lab8/train.csv")
dataset

dataset.info()

X = dataset.drop("satisfaction",axis = 1)
Y = dataset["satisfaction"]

fig = plt.figure(figsize = (20,20))
cnt = 1
for i in dataset.columns[1:]:
  ax = fig.add_subplot(5,5,cnt)
  sns.countplot(data = dataset,x = i,hue = "satisfaction",ax =ax)
  cnt += 1
plt.show()

plt.rcParams["figure.figsize"] = (15,15)
sns.heatmap(dataset.corr(),annot =True,cmap = 'YlGnBu')

"""### Preprocessing"""

X = dataset.drop("satisfaction",axis = 1)
Y = dataset["satisfaction"]
X

Y

for i in ["Gender","Customer Type","Type of Travel","Class"]:
  X[i] = LabelEncoder().fit_transform(X[i])
X
Y = LabelEncoder().fit_transform(Y)

X = X.drop("id",axis = 1)

X.describe()

X.isnull().sum()

plt.rcParams["figure.figsize"] = (5,5)
sns.scatterplot(x = 'Departure Delay in Minutes', y = 'Arrival Delay in Minutes', data = X)

X = X.drop("Departure Delay in Minutes",axis = 1)
median_val = X['Arrival Delay in Minutes'].median()
X['Arrival Delay in Minutes'] = X['Arrival Delay in Minutes'].fillna(median_val)
X.isnull().sum()

fig = plt.figure(figsize = (25,20))
cnt = 1
for i in X.columns:
  ax = fig.add_subplot(5,5,cnt)
  sns.histplot(X[i])
  cnt += 1
plt.show()

YY = LabelEncoder().fit_transform(Y)
sns.histplot(YY)

X.describe()

"""### Part 2"""

model = DecisionTreeClassifier(random_state=42)
sfs = SFS(estimator=model,k_features=10,forward=True,floating=False, scoring='accuracy', cv=5) 

sfs.fit(X, Y)

print('Accuracy with all features:',sfs.k_score_)

print('Best 10 features: {}'.format(sfs.k_feature_names_))

"""### Part 3"""

sfs = SFS(estimator=model,k_features=10,forward=True,floating=False, scoring='accuracy', cv=4) 

sfs.fit(X, Y)

print('Accuracy with all features:',sfs.k_score_)

print('Best 10 features: {}'.format(sfs.k_feature_names_))

sbs = SFS(estimator=model,k_features=10,forward=False,floating=False, scoring='accuracy', cv=4) 

sbs.fit(X, Y)

print('Accuracy with all features:',sbs.k_score_)

print('Best 10 features: {}'.format(sbs.k_feature_names_))

sffs = SFS(estimator=model,k_features=10,forward=True,floating=True, scoring='accuracy', cv=4) 

sffs.fit(X, Y)

print('Accuracy with all features:',sffs.k_score_)

print('Best 10 features: {}'.format(sffs.k_feature_names_))

sbfs = SFS(estimator=model,k_features=10,forward=False,floating=True, scoring='accuracy', cv=4) 

sbfs.fit(X, Y)

print('Accuracy with all features:',sbfs.k_score_)

print('Best 10 features: {}'.format(sbfs.k_feature_names_))

"""### Part 4"""

models = [sfs,sbs,sffs,sbfs]
model_names = ["Sequential forward selection","Sequential Backward Selection","Sequential Forward Floating Selection","Sequential Backward Floating Selection"]

print('For selector =',model_names[0],':\n')

metric_dict = models[0].get_metric_dict()


df = pd.DataFrame.from_dict(metric_dict, orient='index')
df = df[['feature_idx', 'cv_scores', 'avg_score', 'ci_bound']]

df.rename(columns={'feature_idx': 'Feature Subset',
                  'cv_scores': 'CV Scores',
                  'avg_score': 'Mean CV Score',
                  'ci_bound': 'Std CV Score'}, inplace=True)
df

print('For selector =',model_names[1],':\n')

metric_dict = models[1].get_metric_dict()


df = pd.DataFrame.from_dict(metric_dict, orient='index')
df = df[['feature_idx', 'cv_scores', 'avg_score', 'ci_bound']]

df.rename(columns={'feature_idx': 'Feature Subset',
                  'cv_scores': 'CV Scores',
                  'avg_score': 'Mean CV Score',
                  'ci_bound': 'Std CV Score'}, inplace=True)
df

print('For selector =',model_names[2],':\n')

metric_dict = models[2].get_metric_dict()


df = pd.DataFrame.from_dict(metric_dict, orient='index')
df = df[['feature_idx', 'cv_scores', 'avg_score', 'ci_bound']]

df.rename(columns={'feature_idx': 'Feature Subset',
                  'cv_scores': 'CV Scores',
                  'avg_score': 'Mean CV Score',
                  'ci_bound': 'Std CV Score'}, inplace=True)
df

print('For selector =',model_names[3],':\n')

metric_dict = models[3].get_metric_dict()


df = pd.DataFrame.from_dict(metric_dict, orient='index')
df = df[['feature_idx', 'cv_scores', 'avg_score', 'ci_bound']]

df.rename(columns={'feature_idx': 'Feature Subset',
                  'cv_scores': 'CV Scores',
                  'avg_score': 'Mean CV Score',
                  'ci_bound': 'Std CV Score'}, inplace=True)
df

fig = plot_sfs(sfs.get_metric_dict(), kind='std_err')
plt.title('Sequential Forward Selection (SFS)')
plt.ylabel('Accuracy')
plt.xlabel('Number of features')
plt.show()

fig = plot_sfs(sbs.get_metric_dict(), kind='std_err')
plt.title('Sequential Backward Selection (SBS)')
plt.ylabel('Accuracy')
plt.xlabel('Number of features')
plt.show()

fig = plot_sfs(sffs.get_metric_dict(), kind='std_err')
plt.title('Sequential Forward Floating Selection (SFFS)')
plt.ylabel('Accuracy')
plt.xlabel('Number of features')
plt.show()

fig = plot_sfs(sbfs.get_metric_dict(), kind='std_err')
plt.title('Sequential Backward Floating Selection (SFFS)')
plt.ylabel('Accuracy')
plt.xlabel('Number of features')
plt.show()

"""### Part 5"""

def forward_selection(X,Y,similarity_measure):

  best_score = 0
  best_feat = 0
  for i in X.columns:
    score = similarity_measure(pd.DataFrame(X[i]),Y)
    if(best_score < score):
      best_score = score
      best_feat = i

  return best_feat

def backward_selection(X,Y,similarity_measure):
  temp = X
  worst_feat = 0
  worst_score = 100
  for i in X.columns:
    temp = X.drop(i,axis = 1)
    score = similarity_measure(temp,Y)
    if(worst_score > score):
      worst_score = score
      worst_feat = i
  return worst_feat

def bidirectional(X,Y,similarity_measure,k):
  Sf = set()
  Sb = set(list(X.columns))
  n = X.shape[1]
  temp1 = X
  temp2 = X
  while(len(Sf) < k and n-len(Sb) < k):
    Ff = forward_selection(temp1,Y,similarity_measure)
    Fb = backward_selection(temp2,Y,similarity_measure)
    Sf.add(Ff)
    Sb.remove(Fb)
    temp1 = temp1.drop(Ff,axis = 1)
    temp2 = temp2.drop(Fb,axis = 1)
  if(len(Sf) <= k):
    return Sf
  else:
    return Sb

from sklearn.model_selection import GridSearchCV
def DTC_accuracy(X,Y):
  model = DecisionTreeClassifier()
  model.fit(X,Y)
  acc = cvs(model,X,Y,cv = 5).mean()
  return acc
def SVM_accuracy(X, Y):
    model = svm.SVC()
    model.fit(X,Y)
    acc = model.score(X,Y)
    return acc

from sklearn.feature_selection import mutual_info_classif
from scipy.spatial.distance import pdist, squareform
def info_gain(X,Y):
  model = DecisionTreeClassifier()
  model.fit(X,Y)
  ig = mutual_info_classif(X, Y)
  # sc = squareform(ig)
  # score = cvs(model,ig,Y,cv = 5).mean()
  return ig.mean()

def distance_measure_angular(X,Y):
    dt = DecisionTreeClassifier()
    dt.fit(X,Y)
    X = X.to_numpy()
    n_samples = X.shape[0]
    angular_dist = np.zeros((n_samples, n_samples))
    for i in range(n_samples):
        for j in range(n_samples):
            if i == j:
                continue 
            cos_sim = np.dot(X[i], X[j]) / (np.linalg.norm(X[i]) * np.linalg.norm(X[j]))
            if np.isnan(cos_sim):
                angular_dist[i, j] = np.pi
            else:
                angular_dist[i, j] = np.arccos(cos_sim)
    score = cvs(dt, angular_dist, Y, cv=2).mean()
    return score
def distance_measure_euclidean(X,Y):
  dt = DecisionTreeClassifier()
  dt.fit(X,Y)
  X = X.to_numpy()
  n_samples = X.shape[0]
  euclidean_dist = np.zeros((n_samples, n_samples))
  for i in range(n_samples):
      for j in range(n_samples):
          if i == j:
              continue
          euclidean_dist[i, j] = np.linalg.norm(X[i] - X[j])
  score = cvs(dt, euclidean_dist, Y, cv=5).mean()
  return score
def distance_measure_cityblock(X,Y):
  dt = DecisionTreeClassifier()
  dt.fit(X,Y)
  X = X.to_numpy()
  n_samples = X.shape[0]
  cityblock_dist = np.zeros((n_samples, n_samples))

  for i in range(n_samples):
      for j in range(n_samples):
          if i == j:
              continue
          cityblock_dist[i, j] = np.sum(np.abs(X[i] - X[j]))
  score = cvs(dt, cityblock_dist, Y, cv=5).mean()
  return score

cols_dtc = (bidirectional(X.iloc[0:1000],Y[0:1000],DTC_accuracy,10))
cols_dtc

cols_svm = (bidirectional(X.iloc[0:1000],Y[0:1000],SVM_accuracy,10))
cols_svm

cols_city = (bidirectional(X.iloc[0:100],Y[0:100],distance_measure_cityblock,10))
cols_city

cols_eucl = (bidirectional(X.iloc[0:100],Y[0:100],distance_measure_euclidean,10))
cols_eucl

cols_ang = (bidirectional(X.iloc[0:100],Y[0:100],distance_measure_angular,10))
cols_ang

cols_inf = (bidirectional(X.iloc[0:100],Y[0:100],info_gain,10))
cols_inf

"""### Part 7"""

cols_dtc = list(cols_dtc)
temp_dtc = X[cols_dtc]
DTC_accuracy(temp_dtc,Y)

cols_svm = list(cols_svm)
temp_svm = X[cols_svm]
DTC_accuracy(temp_svm.iloc[0:1000],Y[0:1000])

cols_inf = list(cols_inf)
temp_inf = X[cols_inf]
DTC_accuracy(temp_inf,Y)

temp_city = X[list(cols_city)]
distance_measure_cityblock(temp_city.iloc[0:100],Y[0:100])

temp_eucl = X[list(cols_eucl)]
distance_measure_euclidean(temp_eucl.iloc[0:100],Y[0:100])

temp_ang = X[list(cols_ang)]
DTC_accuracy(temp_ang.iloc[0:100],Y[0:100])

"""## Problem 2

### Part 1
"""

cov = np.array([[0.60066771, 0.148898789, 0.244939], [0.148898789, 0.58982531, 0.24154981], [0.244939, 0.24154981, 0.48778655]])
v = np.array([[1/np.sqrt(6)],[1/np.sqrt(6)],[-2/np.sqrt(6)]])
mean = np.array([0,0,0])
data = np.random.multivariate_normal(mean,cov,size = 1000)
labels = np.zeros(1000, dtype=int)
for i in range(len(data)):
  if(data[i].dot(v) > 0):
    labels[i] = 0
  else:
    labels[i] = 1
data = np.c_[data,labels]

dataset = pd.DataFrame(data = data,columns = ["X1","X2","X3","class"])
dataset

traces = []
for i in range(2):
    mask = (labels == i)
    trace = go.Scatter3d(x=data[mask,0], y=data[mask,1], z=data[mask,2],
                         mode='markers', marker=dict(size=2),
                         name='Label {}'.format(i))
    traces.append(trace)

layout = go.Layout(title='3D Scatter Plot', margin=dict(l=0, r=0, b=0, t=0))
fig = go.Figure(data=traces, layout=layout)
fig.show()

"""### Part 2"""

pca = PCA(n_components=3)
pca_data = pca.fit_transform(dataset[["X1","X2","X3"]],)
pca_data = pd.DataFrame(pca_data,columns = [["X1t","X2t","X3t"]])
pca_data["class"] = dataset["class"]
pca_data

"""### Part 3"""

X_train, X_test, Y_train, Y_test = tts(pca_data.drop("class",axis =1), pca_data["class"], test_size=0.3, random_state=42)

X_train = X_train.to_numpy()
Y_train = Y_train.to_numpy()

feature_subsets = [(0, 1), (0, 2), (1, 2)]

for subset in feature_subsets:
    X_subset = X_train[:, subset]
    clf = DecisionTreeClassifier()
    clf.fit(X_subset, Y_train)

    plt.figure()
    plt.scatter(X_subset[:, 0], X_subset[:, 1], c=Y_train, cmap=plt.cm.RdYlBu)
    plt.xlabel('PC{}'.format(subset[0]+1))
    plt.ylabel('PC{}'.format(subset[1]+1))
    plt.title('Decision Boundary for PC{} and PC{}'.format(subset[0]+1, subset[1]+1))
    plot_step = 0.02
    x_min, x_max = X_subset[:, 0].min() - 1, X_subset[:, 0].max() + 1
    y_min, y_max = X_subset[:, 1].min() - 1, X_subset[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),
                         np.arange(y_min, y_max, plot_step))
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu, alpha=0.5)
    plt.show()

"""### Part 4"""

accuracies = {}
for subset in feature_subsets:
    X_subset = X_train[:, subset]
    clf = DecisionTreeClassifier()
    clf.fit(X_subset, Y_train)
    y_pred = clf.predict(X_test.to_numpy()[:, subset])
    accuracies[subset] = accuracy_score(Y_test.to_numpy(), y_pred)

clf_pca = DecisionTreeClassifier()
clf_pca.fit(X_train, Y_train)
y_pred_pca = clf_pca.predict(X_test)
accuracies[tuple((0,1,2))] = accuracy_score(Y_test, y_pred_pca)

for subset, acc in (accuracies.items()):
    print('Subset:', subset, 'Accuracy:', acc)

pca = PCA(n_components = 2)
pca_data_2 = pca.fit(dataset.drop("class",axis = 1))
pca_red = pca.transform(dataset.drop("class",axis = 1))
pca_red

X_train_2, X_test_2, Y_train_2, Y_test_2 = tts(pca_red,dataset["class"], test_size=0.3, random_state=42)

clf_pca_2 = DecisionTreeClassifier()
clf_pca_2.fit(X_train_2, Y_train_2)
y_pred_pca_2 = clf_pca_2.predict(X_test_2)
print(accuracy_score(Y_test_2,y_pred_pca_2))

plt.figure()
plt.scatter(X_train_2[:, 0], X_train_2[:, 1], c=Y_train_2, cmap=plt.cm.RdYlBu)
plot_step = 0.02
x_min, x_max = X_train_2[:, 0].min() - 1, X_train_2[:, 0].max() + 1
y_min, y_max = X_train_2[:, 1].min() - 1, X_train_2[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),
                      np.arange(y_min, y_max, plot_step))
Z = clf_pca_2.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu, alpha=0.5)
plt.show()

dataset.drop("class",axis = 1).cov()

for feature in feature_subsets:
  print(feature)
  dis = np.linalg.norm(X_train[:, feature] -X_train_2,ord = 'fro')
  print(dis,end = "\n")