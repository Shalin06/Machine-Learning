# -*- coding: utf-8 -*-
"""B21CS070_InLab8_Assingment8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uOyBwgcWjjzyJQk4RTdUga3StY5Z3h7Y

# Lab 8
"""

import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split as tts
import random
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder
from sklearn import svm
from sklearn.neighbors import KNeighborsClassifier as KNN
from sklearn.ensemble import BaggingClassifier as BC
from sklearn.model_selection import cross_val_score as cvs
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestClassifier as RFC
from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score,confusion_matrix
from sklearn.naive_bayes import GaussianNB
import warnings
from sklearn.tree import DecisionTreeClassifier
from itertools import combinations
warnings.filterwarnings('ignore')
import plotly.graph_objs as go
from sklearn.decomposition import PCA
!pip install mlxtend
!pip install mlxtend --upgrade --no-deps
from mlxtend.feature_selection import SequentialFeatureSelector as SFS

"""## Problem 2

### Part 1
"""

cov = np.array([[0.60066771, 0.148898789, 0.244939], [0.148898789, 0.58982531, 0.24154981], [0.244939, 0.24154981, 0.48778655]])
v = np.array([[1/np.sqrt(6)],[1/np.sqrt(6)],[-2/np.sqrt(6)]])
mean = np.array([0,0,0])
data = np.random.multivariate_normal(mean,cov,size = 1000)
labels = np.zeros(1000, dtype=int)
for i in range(len(data)):
  if(data[i].dot(v) > 0):
    labels[i] = 0
  else:
    labels[i] = 1
data = np.c_[data,labels]

dataset = pd.DataFrame(data = data,columns = ["X1","X2","X3","class"])
dataset

traces = []
for i in range(2):
    mask = (labels == i)
    trace = go.Scatter3d(x=data[mask,0], y=data[mask,1], z=data[mask,2],
                         mode='markers', marker=dict(size=2),
                         name='Label {}'.format(i))
    traces.append(trace)

layout = go.Layout(title='3D Scatter Plot', margin=dict(l=0, r=0, b=0, t=0))
fig = go.Figure(data=traces, layout=layout)
fig.show()

"""### Part 2"""

pca = PCA(n_components=3)
pca_data = pca.fit_transform(dataset[["X1","X2","X3"]],)
pca_data = pd.DataFrame(pca_data,columns = [["X1t","X2t","X3t"]])
pca_data["class"] = dataset["class"]
pca_data

"""### Part 3"""

X_train, X_test, Y_train, Y_test = tts(pca_data.drop("class",axis =1), pca_data["class"], test_size=0.3, random_state=42)

X_train = X_train.to_numpy()
Y_train = Y_train.to_numpy()

feature_subsets = [(0, 1), (0, 2), (1, 2)]

for subset in feature_subsets:
    X_subset = X_train[:, subset]
    clf = DecisionTreeClassifier()
    clf.fit(X_subset, Y_train)

    plt.figure()
    plt.scatter(X_subset[:, 0], X_subset[:, 1], c=Y_train, cmap=plt.cm.RdYlBu)
    plt.xlabel('PC{}'.format(subset[0]+1))
    plt.ylabel('PC{}'.format(subset[1]+1))
    plt.title('Decision Boundary for PC{} and PC{}'.format(subset[0]+1, subset[1]+1))
    plot_step = 0.02
    x_min, x_max = X_subset[:, 0].min() - 1, X_subset[:, 0].max() + 1
    y_min, y_max = X_subset[:, 1].min() - 1, X_subset[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),
                         np.arange(y_min, y_max, plot_step))
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu, alpha=0.5)
    plt.show()

"""### Part 4"""

accuracies = {}
for subset in feature_subsets:
    X_subset = X_train[:, subset]
    clf = DecisionTreeClassifier()
    clf.fit(X_subset, Y_train)
    y_pred = clf.predict(X_test.to_numpy()[:, subset])
    accuracies[subset] = accuracy_score(Y_test.to_numpy(), y_pred)

clf_pca = DecisionTreeClassifier()
clf_pca.fit(X_train, Y_train)
y_pred_pca = clf_pca.predict(X_test)
accuracies[tuple((0,1,2))] = accuracy_score(Y_test, y_pred_pca)

for subset, acc in (accuracies.items()):
    print('Subset:', subset, 'Accuracy:', acc)

pca = PCA(n_components = 2)
pca_data_2 = pca.fit(dataset.drop("class",axis = 1))
pca_red = pca.transform(dataset.drop("class",axis = 1))
pca_red

X_train_2, X_test_2, Y_train_2, Y_test_2 = tts(pca_red,dataset["class"], test_size=0.3, random_state=42)

clf_pca_2 = DecisionTreeClassifier()
clf_pca_2.fit(X_train_2, Y_train_2)
y_pred_pca_2 = clf_pca_2.predict(X_test_2)
print(accuracy_score(Y_test_2,y_pred_pca_2))

plt.figure()
plt.scatter(X_train_2[:, 0], X_train_2[:, 1], c=Y_train_2, cmap=plt.cm.RdYlBu)
plot_step = 0.02
x_min, x_max = X_train_2[:, 0].min() - 1, X_train_2[:, 0].max() + 1
y_min, y_max = X_train_2[:, 1].min() - 1, X_train_2[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),
                      np.arange(y_min, y_max, plot_step))
Z = clf_pca_2.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu, alpha=0.5)
plt.show()

dataset.drop("class",axis = 1).cov()

for feature in feature_subsets:
  print(feature)
  dis = np.linalg.norm(X_train[:, feature] -X_train_2,ord = 'fro')
  print(dis,end = "\n")

"""We can see that the distance between the same matrix components is very less (~0) for the case of feature(0,1) hence on applying n_componets = 2 and 3 we get similar accuracy for this particular case. And for the other cases we have high accuracy due to high distances."""