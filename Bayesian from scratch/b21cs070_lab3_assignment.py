# -*- coding: utf-8 -*-
"""B21CS070_LAB3_ASSIGNMENT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GPhsw-EzcmlqTNPT5MRvhkDWxbkIKx3z

# Lab 3
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder as lb
from sklearn.preprocessing import normalize
from sklearn.model_selection import train_test_split as tts
from sklearn.tree import DecisionTreeRegressor as DTR
from matplotlib import pyplot as plt
from sklearn.metrics import mean_squared_error as mse
from sklearn.tree import DecisionTreeClassifier as DTC
from sklearn.model_selection import cross_val_score as cvs
from sklearn.model_selection import KFold
from sklearn.model_selection import RepeatedKFold
from sklearn import tree
import math
from sklearn.linear_model import LogisticRegression as LR
from matplotlib.ticker import AutoMinorLocator
import seaborn as sns
from sklearn import naive_bayes as nb;
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from sklearn.metrics import plot_roc_curve
from sklearn.metrics import precision_score,recall_score,f1_score
from sklearn.metrics import confusion_matrix

"""## Problem 1

### 1. Perform pre-processing and visualization of the dataset. Split the data into train and test sets. Also identify the useful columns and drop the unnecessary ones
"""

titan = pd.read_csv("/content/drive/MyDrive/PRML/Lab3/titanic.csv")
titanic_dataset = pd.DataFrame(titan,columns = titan.columns)
 
titanic_dataset_new = titanic_dataset.copy()
titanic_dataset_new=titanic_dataset_new.drop(labels=["PassengerId","Name","Ticket","Cabin"],axis=1)
titanic_dataset_new

"""#### Preprocessing the dataset"""

X = titanic_dataset_new.copy()
lb_new = lb()
X = X.drop(X[(X["Age"] < 1)].index)
X =X.dropna()
X["Age"] = X["Age"].astype(int)
# X["Sex"] = lb.fit_transform(X["Sex"])
Y = X["Survived"]
# X = X.drop(labels = ["Survived"],axis = 1)

X["Sex"] = lb_new.fit_transform(X["Sex"])
X["Embarked"] = lb_new.fit_transform(X["Embarked"])

X = X.set_index(np.arange(len(X)))
X

"""#### Visualization of data

"""

plt.rcParams['figure.figsize'] = [5,5]
s = sns.catplot(x="Pclass",hue="Survived",kind="count",data = X,legend=(True))

sns.catplot(x="Sex",hue="Survived",kind="count",data = X,legend=(True))

sns.violinplot(x="Sex",y = "Age",hue="Survived",data = X,split = True)

X = X.drop(["Survived","Pclass"],axis = 1)
sns.heatmap(data=X.corr(),annot=True)

sns.distplot(X["Age"])
plt.show()
sns.distplot(X["Fare"])

"""#### Splitting of data"""

X_train,X_test,Y_train,Y_test= tts(X,Y,test_size = 0.2,shuffle=True)

"""### 2) Identify the best possible variant of naive bayes classifier for the given dataset. Justify your reason for the same

"""

naivebayesvariant = [nb.GaussianNB,nb.CategoricalNB,nb.ComplementNB,nb.MultinomialNB,nb.BernoulliNB]
for i in naivebayesvariant:
  model = i()
  model.fit(X_train,Y_train)
  print(str(model),"-->",mse(Y_test,model.predict(X_test)))
model_gauss = nb.GaussianNB()
model_gauss.fit(X_train[["Age","Fare"]],Y_train)
model_cat = nb.CategoricalNB()
model_cat.fit(X_train[["Sex","Embarked"]],Y_train)
Y_gauss_probabilities = model_gauss.predict_proba(X_test[["Age","Fare"]])
Y_cat_probabilities = model_cat.predict_proba(X_test[["Sex","Embarked"]])
Y_final_prob = np.multiply(Y_gauss_probabilities,Y_cat_probabilities)
final_y=np.zeros(len(Y_final_prob))
for i in range(len(final_y)):
  if(Y_final_prob[i][1]>Y_final_prob[i][0]):
    final_y[i]=1
print("mse on combining categorical and gaussian",mse(final_y,Y_test))

"""### 3) Implement the identified variant of Naive Bayes Classifier using scikit learn,report its performance based on appropriate metrics.(ROC AUC etc)"""

plt.rcParams['figure.figsize'] = [5, 5]
final_model = nb.GaussianNB()
final_model.fit(X_train,Y_train)
print(final_model.score(X_test,Y_test))
plot_roc_curve(final_model,X_test,Y_test)
print("Confusion Matrix -->")
print(confusion_matrix(Y_test,final_model.predict(X_test)))
print("Precision Score -->",precision_score(Y_test,final_model.predict(X_test)))
print("Recall Score -->",recall_score(Y_test,final_model.predict(X_test)))
print("F1-Score -->",f1_score(Y_test,final_model.predict(X_test)))

"""### 4).Perform 5 fold cross validation and summarize the results across the cross-validation sets. Compute the probability of the top class for each row in the testing dataset."""

kfold = KFold(5)
result = cvs(final_model,X,Y,cv=kfold)
print(result.mean())
Probabilities = final_model.predict_proba(X_test)
Topclass = []
for i in range(len(Probabilities)):
  Topclass.append(max(Probabilities[i][0],Probabilities[i][1]))
print(Topclass)

"""### 5) Make contour plots with the data points to visualize the class-conditional densities. What can you say about the assumption Naive Bayes model is based on from these plots? Explain in your report"""

sns.kdeplot(data = titanic_dataset,x="Age",y="Fare",hue="Survived")
X_new = titanic_dataset.copy()
X_new["Sex"] = lb_new.fit_transform(X_new["Sex"])
X_new["Embarked"] = lb_new.fit_transform(X_new["Embarked"])
plt.show()
sns.kdeplot(data = X_new,x="Fare",y="Sex",hue="Survived")
plt.show()
sns.kdeplot(data = X_new,x="Age",y="Sex",hue="Survived")

"""### 6) Compare your model with the Decision Tree classifier on the same dataset by performing 5-fold cross-validation and summarizing the results. Justify why one of them works better on this numeric dataset."""

model_DTC = DTC()
model_DTC.fit(X_train,Y_train)
kfold = KFold(5)
result_DTC =cvs(model_DTC,X,Y,cv=kfold)
print("Model_DTC score -->",result_DTC.mean())
print("Model_Gaussian score -->", result.mean())

"""## Problem 2

"""

data = pd.read_csv("/content/drive/MyDrive/PRML/Lab3/dataset (1).csv")
dataf = pd.DataFrame(data = data)
X = dataf.copy()
Y = dataf["Y"]
X = X.drop("Y",axis = 1)
X = X.dropna()
X

"""### 1) Use histogram to plot the distribution of samples"""

for i in X.columns:
  plt.rcParams['figure.figsize'] = [20, 5]
  x1 = list(dataf[dataf['Y'] == 1][i])
  x2 = list(dataf[dataf['Y'] == 2][i])
  x3 = list(dataf[dataf['Y'] == 3][i])

  colors=['blue', 'green', 'orange']
  names=['1', '2', '3']
  n,bins,patches=plt.hist([x1, x2, x3], color=colors, label=names,bins = 7, density=False)
  plt.legend()

  minor_locator = AutoMinorLocator(2)
  plt.gca().xaxis.set_minor_locator(minor_locator)
  plt.grid(which='minor', color='white', lw = 0.5)

  xticks = [(bins[idx+1] + value)/2 for idx, value in enumerate(bins[:-1])]
  xticks_labels = [ "{:.2f} to {:.2f}".format(value, bins[idx+1]) for idx, value in enumerate(bins[:-1])]
  plt.xticks(xticks, labels = xticks_labels)
  plt.xlabel(i)
  plt.ylabel("Count")
  plt.show()
  print('\n','\n')

"""### 2) Determine the prior probability for all the classes. """

def Prior_Prob_classes():
  out = {}
  for i in range(1,4):
    x = np.count_nonzero(Y==i)/len(dataf["Y"])
    out[i] = x
  return out
Prior_Prob_classes()

"""### 3) . Discretize the features into bins from scratch. Use of pandas, scikit learn and scipy is not allowed for this subpart."""

dataf_new = dataf.copy()
def Make_bins(no_of_bins):
  for i in dataf_new.columns:
    min_val = dataf_new[i].min();
    max_val = dataf_new[i].max();
    diff = (max_val - min_val)/no_of_bins
    if(i == "Y"):
        continue;
    for j in range(len(dataf_new[i])):
      for k in range(1,no_of_bins + 1):
        if(dataf_new[i][j] <= min_val + diff*k):
          dataf_new[i][j] = k
          break;
Make_bins(7)
dataf_new = dataf_new.astype(int)
dataf_new

"""### 4) Determine the likelihood/class conditional probabilities for all the classes. """

def likelihood(feature,class_no,bin_no):
    index = np.where(dataf_new["Y"]==class_no)
    cnt= len(dataf_new[(dataf_new['Y']==class_no)&(dataf_new[feature]==bin_no)].index)
    prob = cnt/len(index[0])
    return prob;
def likelihood_of_paricular_feature(feature,no_bins,class_no):
      d   = {}
      for j in range(1,no_bins+1):
        d[j] = likelihood(feature,class_no,j)
      return d

def probabiblity_of_class():
    for i in range(1,4):
        for  j in dataf_new.columns:
          if(j=="Y"):
            continue;
          else:
            d = likelihood_of_paricular_feature(j,7,i)
            print("Class",i,j)
            print(d)
probabiblity_of_class()

"""### 5)  Plot the count of each unique element for each class. Compare the plot with the plot of distribution."""

plt.rcParams['figure.figsize'] = [5, 5]

for i in range(1,4):
  no_bins = [1,2,3,4,5,6,7]
  for k in dataf_new.columns:
    XY = []
    if(k=='Y'):
      continue
    else:
      for j in no_bins:
        cnt = len(dataf_new[(dataf_new['Y']==i)&(dataf_new[k]==j)].index)
        XY.append(cnt)
      # plt.subplot(3,3,7)
      plt.bar(no_bins,XY)
      plt.title("Class " + str(i)+" "+"feature " + k)
      plt.xlabel("No. of bins")
      plt.ylabel("Count")
      plt.show()

"""### 6) Calculate the posterior probabilities and plot them in a single graph. Analyze the plot"""

X_2 = dataf_new.copy()
Y_2 = dataf_new["Y"]
# X_2 = X_2.drop("Y",axis = 1)
X_2_train,X_2_test,Y_2_train,Y_2_test = tts(X_2,Y_2,test_size=0.2,shuffle=True)  
print(Y_2_train.shape,Y_2_test.shape)

plt.rcParams['figure.figsize'] = [20, 5]
def newlikelihood(feature,class_no,bin_no,dataset):
    index = np.where(dataset["Y"]==class_no)
    cnt= len(dataset[(dataset['Y']==class_no)&(dataset[feature]==bin_no)].index)
    if(cnt == 0):
      prob = 0
    else:
      prob = cnt/len(index[0])
    return prob;
def posterior_prob_likelihood(class_no,sample):
  prob = 1;
  for i in range(7):
    # if(i=="Y"):
    #   x = 1;
    # else:
      prob *= newlikelihood(X_2_train.columns[i],class_no,sample[i],X_2_train)
  out =Prior_Prob_classes()
  prob *= (out[class_no])
  return prob;
def probabilities():
  x_axis = []
  out1 = []
  out2 = []
  out3 = []
  for i in range(len(X_2_test.index)):
    x_axis.append(i)
  for j in X_2_test.index:
      for k in range(1,4):
        if(k==1):
          out1.append(posterior_prob_likelihood(k,X_2_test.loc[j]))
        elif(k==2):
          out2.append(posterior_prob_likelihood(k,X_2_test.loc[j]))
        elif(k==3):
          out3.append(posterior_prob_likelihood(k,X_2_test.loc[j]))
  for i in range(len(out1)):
    s = out1[i] + out2[i] + out3[i]
    if(s != 0):
      out1[i] = out1[i]/s
      out2[i] = out2[i]/s
      out3[i] = out3[i]/s
  # print(out1)
  # print(out2)
  # print(out3)
  plt.scatter(x_axis, out1, color='r', label='class 1')
  plt.scatter(x_axis, out2, color='b', label='class 2')
  plt.scatter(x_axis, out3, color='g', label='class 3')
  plt.legend()
  plt.show()

probabilities()